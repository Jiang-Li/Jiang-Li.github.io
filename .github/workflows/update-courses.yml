name: Update Course Data
# Trigger GitHub Actions scheduler (updated Oct 30, 2025)
on:
  push:
    branches: [master]
  schedule:
    - cron: '0 11 * * *'  # Run daily at 6 AM EST (11 AM UTC)
  workflow_dispatch:

jobs:
  update-course-report:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pages: write
      id-token: write

    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}

    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 2  # Needed to detect file changes

    # NEW: Detect what files changed to decide if scraping is needed
    - name: Detect changes
      id: changes
      run: |
        if [ "${{ github.event_name }}" = "schedule" ] || [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
          echo "scrape=true" >> $GITHUB_OUTPUT
          echo "🕒 Scheduled/Manual run: Will run course scraper"
        else
          # Check if course-related files changed in this push
          CHANGED_FILES=$(git diff --name-only HEAD~1 HEAD)
          echo "📝 Changed files in this push:"
          echo "$CHANGED_FILES"
          
          if echo "$CHANGED_FILES" | grep -E "(teach/franklin-course-scraper/|teach/course-schedule\.qmd|course_request\.md)"; then
            echo "scrape=true" >> $GITHUB_OUTPUT
            echo "🎯 Course-related files changed: Will run course scraper"
          else
            echo "scrape=false" >> $GITHUB_OUTPUT
            echo "📄 Only non-course files changed: Skipping course scraper for faster build"
          fi
        fi

    # Always setup Python since course-schedule.qmd contains Python code
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.9'

    - name: Install basic Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pandas numpy jupyter nbclient nbconvert ipykernel

    # Additional dependencies only needed for scraping
    - name: Install scraping dependencies
      if: steps.changes.outputs.scrape == 'true'
      run: |
        pip install beautifulsoup4 selenium

    - name: Install Chrome and ChromeDriver
      if: steps.changes.outputs.scrape == 'true'
      run: |
        # Install Chrome using modern keyring approach
        wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo gpg --dearmor -o /usr/share/keyrings/google-chrome-keyring.gpg
        echo "deb [arch=amd64 signed-by=/usr/share/keyrings/google-chrome-keyring.gpg] http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
        sudo apt-get update
        sudo apt-get install -y google-chrome-stable
        
        # Install ChromeDriver using webdriver-manager approach
        pip install webdriver-manager
        
        # Verify Chrome installation
        google-chrome --version

    - name: Scrape Franklin course data
      if: steps.changes.outputs.scrape == 'true'
      run: |
        echo "🎯 Franklin Course Report Update - Starting Data Collection"
        echo "============================================================"
        cd teach/franklin-course-scraper
        python scripts/scrape_franklin_courses.py || echo "⚠️ Course scraping failed, continuing with existing data..."
        
        # Check if fresh data was created
        if [ -f "data/franklin_courses.csv" ]; then
          echo "✅ Fresh course data generated successfully"
          echo "📊 File size: $(du -h data/franklin_courses.csv)"
          echo "📋 Course sections found: $(($(wc -l < data/franklin_courses.csv) - 1))"
          echo "🕒 Data timestamp: $(date)"
        else
          echo "⚠️ No course data file found - course schedule will show error message"
        fi

    # Always setup Quarto regardless of scraping
    - name: Set up Quarto
      uses: quarto-dev/quarto-actions/setup@v2

    # Smart rebuild logic based on trigger type and file changes
    - name: Check if full rebuild needed
      id: check-rebuild
      run: |
        if [ "${{ github.event_name }}" = "schedule" ]; then
          echo "rebuild=false" >> $GITHUB_OUTPUT
          echo "📄 Scheduled run: Only course schedule updated"
        else
          echo "rebuild=true" >> $GITHUB_OUTPUT  
          echo "🔄 Push/Manual run: Full site rebuild"
        fi

    # Always render course schedule if scraping occurred (to use fresh data)
    - name: Render course schedule (after scraping)
      if: steps.changes.outputs.scrape == 'true'
      run: |
        echo "📊 Rendering course schedule with fresh scraped data..."
        echo "📁 Current directory: $(pwd)"
        echo "📄 Verifying CSV file exists with fresh data:"
        ls -lh teach/franklin-course-scraper/data/franklin_courses.csv || echo "⚠️ CSV not found!"
        if [ -f "teach/franklin-course-scraper/data/franklin_courses.csv" ]; then
          echo "📊 CSV info:"
          echo "  - File size: $(du -h teach/franklin-course-scraper/data/franklin_courses.csv | cut -f1)"
          echo "  - Row count: $(($(wc -l < teach/franklin-course-scraper/data/franklin_courses.csv) - 1))"
          echo "  - Last timestamp: $(tail -1 teach/franklin-course-scraper/data/franklin_courses.csv | cut -d',' -f17 || echo 'N/A')"
        fi
        echo "🔄 Rendering course schedule (path detection will find CSV automatically)..."
        quarto render teach/course-schedule.qmd || (echo "❌ Render failed!" && exit 1)
        echo "✅ Course schedule rendered successfully"
        echo "📊 Verifying output:"
        ls -lh _site/teach/course-schedule.html || (echo "⚠️ Output file not found!" && exit 1)

    - name: Render full website (if needed)
      if: steps.check-rebuild.outputs.rebuild == 'true'
      run: |
        echo "🌐 Rendering complete website..."
        quarto render
        echo "✅ Full website rendered successfully"

    - name: Ensure site directory exists
      run: |
        # If we scraped but didn't rebuild, we already rendered course schedule
        # Just ensure _site directory exists
        if [ "${{ steps.changes.outputs.scrape }}" = "true" ] && [ "${{ steps.check-rebuild.outputs.rebuild }}" = "false" ]; then
          echo "📊 Scheduled run: Course schedule already rendered, verifying _site..."
          if [ ! -d "_site" ]; then
            echo "⚠️ _site directory missing, rendering course schedule..."
            quarto render teach/course-schedule.qmd
          fi
        elif [ "${{ steps.check-rebuild.outputs.rebuild }}" = "false" ]; then
          echo "🌐 No scraping and no rebuild: Rendering full site..."
          quarto render
        else
          echo "✅ Site directory should already exist from previous steps"
        fi

    - name: Setup Pages
      uses: actions/configure-pages@v4

    - name: Upload site
      uses: actions/upload-pages-artifact@v3
      with:
        path: _site

    - name: Deploy to GitHub Pages
      id: deployment
      uses: actions/deploy-pages@v4

    - name: Report deployment success
      run: |
        echo "🎉 Course report update completed successfully!"
        if [ "${{ steps.changes.outputs.scrape }}" = "true" ]; then
          echo "📊 Fresh course data collected and processed"
        else
          echo "📄 Quick update: Non-course files updated only"
        fi
        echo "🌐 Website deployed to: ${{ steps.deployment.outputs.page_url }}"
        echo "📅 Course schedule available at: ${{ steps.deployment.outputs.page_url }}teach/course-schedule.html" 