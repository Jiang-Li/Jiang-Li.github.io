name: Update Course Data
on:
  push:
    branches: [master]
  schedule:
    - cron: '0 11 * * *'  # Run daily at 6 AM EST (11 AM UTC)
  workflow_dispatch:

jobs:
  update-course-report:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pages: write
      id-token: write

    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}

    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 2  # Needed to detect file changes

    # NEW: Detect what files changed to decide if scraping is needed
    - name: Detect changes
      id: changes
      run: |
        if [ "${{ github.event_name }}" = "schedule" ] || [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
          echo "scrape=true" >> $GITHUB_OUTPUT
          echo "ğŸ•’ Scheduled/Manual run: Will run course scraper"
        else
          # Check if course-related files changed in this push
          CHANGED_FILES=$(git diff --name-only HEAD~1 HEAD)
          echo "ğŸ“ Changed files in this push:"
          echo "$CHANGED_FILES"
          
          if echo "$CHANGED_FILES" | grep -E "(teach/franklin-course-scraper/|teach/course-schedule\.qmd|course_request\.md)"; then
            echo "scrape=true" >> $GITHUB_OUTPUT
            echo "ğŸ¯ Course-related files changed: Will run course scraper"
          else
            echo "scrape=false" >> $GITHUB_OUTPUT
            echo "ğŸ“„ Only non-course files changed: Skipping course scraper for faster build"
          fi
        fi

    # Always setup Python since course-schedule.qmd contains Python code
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.9'

    - name: Install basic Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pandas numpy jupyter nbclient nbconvert ipykernel

    # Additional dependencies only needed for scraping
    - name: Install scraping dependencies
      if: steps.changes.outputs.scrape == 'true'
      run: |
        pip install requests beautifulsoup4 selenium

    - name: Install Chrome and ChromeDriver
      if: steps.changes.outputs.scrape == 'true'
      run: |
        # Install Chrome using modern keyring approach
        wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo gpg --dearmor -o /usr/share/keyrings/google-chrome-keyring.gpg
        echo "deb [arch=amd64 signed-by=/usr/share/keyrings/google-chrome-keyring.gpg] http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
        sudo apt-get update
        sudo apt-get install -y google-chrome-stable
        
        # Install ChromeDriver using webdriver-manager approach
        pip install webdriver-manager
        
        # Verify Chrome installation
        google-chrome --version

    - name: Create data directory
      if: steps.changes.outputs.scrape == 'true'
      run: |
        mkdir -p teach/franklin-course-scraper/data

    - name: Scrape Franklin course data
      if: steps.changes.outputs.scrape == 'true'
      run: |
        echo "ğŸ¯ Franklin Course Report Update - Starting Data Collection"
        echo "============================================================"
        cd teach/franklin-course-scraper
        timeout 300 python scripts/scrape_franklin_courses.py || echo "âš ï¸ Course scraping failed or timed out, continuing with existing data..."
        
        # Check if fresh data was created
        if [ -f "data/franklin_courses.csv" ]; then
          echo "âœ… Fresh course data generated successfully"
          echo "ğŸ“Š File size: $(du -h data/franklin_courses.csv)"
          echo "ğŸ“‹ Course sections found: $(($(wc -l < data/franklin_courses.csv) - 1))"
          echo "ğŸ•’ Data timestamp: $(date)"
        else
          echo "âš ï¸ No course data file found - course schedule will show error message"
        fi

    - name: Verify course data quality
      if: steps.changes.outputs.scrape == 'true'
      run: |
        if [ -f teach/franklin-course-scraper/data/franklin_courses.csv ]; then
          echo "ğŸ” Data Quality Check:"
          echo "ğŸ“„ File exists: âœ…"
          echo "ğŸ“ File size: $(stat -c%s teach/franklin-course-scraper/data/franklin_courses.csv) bytes"
          echo "ğŸ“Š Total rows: $(wc -l < teach/franklin-course-scraper/data/franklin_courses.csv)"
          echo "ğŸ“ Sample data (first row):"
          head -n 2 teach/franklin-course-scraper/data/franklin_courses.csv | tail -n 1 | cut -d',' -f1-4
        else
          echo "âŒ Course data file not found - checking directory structure:"
          find teach/franklin-course-scraper -name "*.csv" -type f || echo "No CSV files found"
        fi

    # Always setup Quarto regardless of scraping
    - name: Set up Quarto
      uses: quarto-dev/quarto-actions/setup@v2

    # Only render course schedule if we scraped new data
    - name: Render course schedule
      if: steps.changes.outputs.scrape == 'true'
      run: |
        echo "ğŸ“ Rendering course schedule with fresh data..."
        echo "Course data path: teach/franklin-course-scraper/data/franklin_courses.csv"
        quarto render teach/course-schedule.qmd
        echo "âœ… Course schedule rendered successfully"

    # Smart rebuild logic based on trigger type and file changes
    - name: Check if full rebuild needed
      id: check-rebuild
      run: |
        if [ "${{ github.event_name }}" = "schedule" ]; then
          echo "rebuild=false" >> $GITHUB_OUTPUT
          echo "ğŸ“„ Scheduled run: Only course schedule updated"
        else
          echo "rebuild=true" >> $GITHUB_OUTPUT  
          echo "ğŸ”„ Push/Manual run: Full site rebuild"
        fi

    - name: Render full website (if needed)
      if: steps.check-rebuild.outputs.rebuild == 'true'
      run: |
        echo "ğŸŒ Rendering complete website..."
        quarto render
        echo "âœ… Full website rendered successfully"

    - name: Ensure site directory exists
      run: |
        # Make sure _site directory exists for deployment
        if [ ! -d "_site" ]; then
          echo "ğŸ”§ Creating _site directory and rendering full site..."
          quarto render
        else
          echo "âœ… Site directory exists"
        fi

    - name: Setup Pages
      uses: actions/configure-pages@v4

    - name: Upload site
      uses: actions/upload-pages-artifact@v3
      with:
        path: _site

    - name: Deploy to GitHub Pages
      id: deployment
      uses: actions/deploy-pages@v4

    - name: Report deployment success
      run: |
        echo "ğŸ‰ Course report update completed successfully!"
        if [ "${{ steps.changes.outputs.scrape }}" = "true" ]; then
          echo "ğŸ“Š Fresh course data collected and processed"
        else
          echo "ğŸ“„ Quick update: Non-course files updated only"
        fi
        echo "ğŸŒ Website deployed to: ${{ steps.deployment.outputs.page_url }}"
        echo "ğŸ“… Course schedule available at: ${{ steps.deployment.outputs.page_url }}teach/course-schedule.html" 