name: Update Course Data
on:
  push:
    branches: [master]
  schedule:
    - cron: '0 6 * * *'  # Run daily at 6 AM UTC (2 AM EDT)
  workflow_dispatch:

jobs:
  simple-deploy:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pages: write
      id-token: write

    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.9'

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests beautifulsoup4 pandas numpy PyYAML jupyter nbclient nbconvert ipykernel selenium

    - name: Install Chrome and ChromeDriver
      run: |
        # Install Chrome
        wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
        echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
        sudo apt-get update
        sudo apt-get install -y google-chrome-stable
        
        # Install ChromeDriver
        CHROME_VERSION=$(google-chrome --version | grep -oP '\d+\.\d+\.\d+')
        CHROMEDRIVER_VERSION=$(curl -s "https://chromedriver.storage.googleapis.com/LATEST_RELEASE_${CHROME_VERSION%%.*}")
        wget -q "https://chromedriver.storage.googleapis.com/${CHROMEDRIVER_VERSION}/chromedriver_linux64.zip"
        unzip chromedriver_linux64.zip
        sudo mv chromedriver /usr/local/bin/
        sudo chmod +x /usr/local/bin/chromedriver
        
        # Verify installation
        google-chrome --version
        chromedriver --version

    - name: Create data directory
      run: |
        mkdir -p teach/franklin-course-scraper/data

    - name: Scrape Franklin course data
      run: |
        echo "üîç Starting course data scraping..."
        cd teach/franklin-course-scraper
        timeout 300 python scripts/scrape_franklin_courses.py || echo "‚ö†Ô∏è Course scraping failed or timed out, continuing with existing data..."
        
        # Check if data was created
        if [ -f "data/franklin_courses.csv" ]; then
          echo "‚úÖ Course data file exists"
          wc -l data/franklin_courses.csv
        else
          echo "‚ö†Ô∏è No course data file found, website will show error message"
        fi

    - name: Verify course data
      run: |
        if [ -f teach/franklin-course-scraper/data/franklin_courses.csv ]; then
          echo "‚úÖ Course data file exists"
          echo "üìä File size: $(du -h teach/franklin-course-scraper/data/franklin_courses.csv)"
          echo "üìã Lines in file: $(wc -l < teach/franklin-course-scraper/data/franklin_courses.csv)"
          echo "üîç First few lines:"
          head -n 3 teach/franklin-course-scraper/data/franklin_courses.csv
        else
          echo "‚ùå Course data file not found"
          ls -la teach/franklin-course-scraper/data/ || echo "Data directory not found"
        fi

    - name: Set up Quarto
      uses: quarto-dev/quarto-actions/setup@v2

    - name: Render website
      run: |
        echo "Rendering Quarto website..."
        quarto render

    - name: Setup Pages
      uses: actions/configure-pages@v4

    - name: Upload site
      uses: actions/upload-pages-artifact@v3
      with:
        path: _site

    - name: Deploy to GitHub Pages
      id: deployment
      uses: actions/deploy-pages@v4 